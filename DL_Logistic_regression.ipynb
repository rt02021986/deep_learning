{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is to build logistic regression using Neural network, Build cat vs dog classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impoet necessary libraires\n",
    "import numpy as np\n",
    "# import math\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom variables\n",
    "cat = 0\n",
    "dog = 1\n",
    "height = 130\n",
    "width = 130\n",
    "\n",
    "cat_train_filepath = 'data/cat_dog_ds/train/cats/*'\n",
    "dog_train_filepath = 'data/cat_dog_ds/train/dogs/*'\n",
    "cat_test_filepath = 'data/cat_dog_ds/test/cats/*'\n",
    "dog_test_filepath = 'data/cat_dog_ds/test/dogs/*'\n",
    "\n",
    "#define W and b metrices\n",
    "# W shape would be (1,height*width*3)\n",
    "# b would be scaler\n",
    "# lr is learning rate\n",
    "\n",
    "W = np.random.rand(height*width*3,1)\n",
    "b = np.random.rand(1)\n",
    "b\n",
    "lr = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function which read train cats and dog dataset and create training dataset with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the min dimension of the image\n",
    "def create_x_y_set(filepath,height,width,label):\n",
    "    down_width = width\n",
    "    down_height = height\n",
    "    down_points = (down_width, down_height)\n",
    "    pics = glob.glob(filepath)\n",
    "    train_images = []\n",
    "    for pic in pics:\n",
    "        img = cv2.imread(pic, cv2.IMREAD_COLOR)\n",
    "        resized_down = cv2.resize(img, down_points, interpolation= cv2.INTER_LINEAR)\n",
    "        cat_img = resized_down.reshape(height*width*3,1)/255\n",
    "        train_images.append(cat_img)\n",
    "    train_images = np.array(train_images).reshape(height*width*3,len(pics))\n",
    "    train_label = np.array([label] * len(pics)).reshape(1,len(pics))\n",
    "    return train_images, train_label\n",
    "\n",
    "#define sigmoid function\n",
    "def sigmoid_fun(x):\n",
    "    s = 1/(1+ np.exp(-x))\n",
    "    return s\n",
    "\n",
    "# Sigmoid gradient\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid_fun(x)\n",
    "    ds = s*(1-s)\n",
    "    return ds\n",
    "\n",
    "# Implement L1 & L2 loss function\n",
    "# L1(y,y_hat) = summation (i to m) abs(y(i) - y_hat(i))\n",
    "\n",
    "def l1_loss(yhat, y):\n",
    "    loss = sum(abs(yhat - y))\n",
    "    return loss\n",
    "\n",
    "def l2_loss(yhat, y):\n",
    "    loss = sum((yhat-y)**2)\n",
    "    return loss\n",
    "\n",
    "def l2_loss_new(yhat, y):\n",
    "    x = yhat-y\n",
    "    loss = np.dot(x,x)\n",
    "    return loss\n",
    "\n",
    "def binary_entropy_loss(yhat, y):\n",
    "    m = y.shape[1]\n",
    "    yhat = np.clip(yhat, 1e-7, 1 - 1e-7)\n",
    "    loss = -np.mean(((y*np.log(yhat))+(1-y)*np.log(1-yhat)))\n",
    "    # loss = (np.dot(y,np.log(yhat))+np.dot(1-y,np.log(1-yhat)))\n",
    "    return loss\n",
    "\n",
    "# da is nothing but derivative of loss with respect to a = sigmoid(z), z = W.T*X+B\n",
    "def partial_derivative_da(yhat,y):\n",
    "    da = (-y/yhat) + (1-y)/(1-yhat)\n",
    "    return da\n",
    "\n",
    "def partial_derivative_dz(yhat,y):\n",
    "    dz = (yhat-y)\n",
    "    return dz\n",
    "\n",
    "def partial_derivative_dw(dz,X):\n",
    "    # print(f' dz shape - {dz.shape}')\n",
    "    # print(f' X shape - {X.shape}')\n",
    "    # print(f' X shape(1) - {X.shape[1]}')\n",
    "    dw = np.dot(X,dz.transpose())/X.shape[1]\n",
    "    # print(f' dw shape - {dw.shape}')\n",
    "    return dw\n",
    "\n",
    "def partial_derivative_db(dz):\n",
    "    db = np.sum(dz)/dz.shape[1]\n",
    "    return db\n",
    "\n",
    "#forward pass\n",
    "# Z = W.T*X+b\n",
    "\n",
    "def calculate_yhat(X,W,b):\n",
    "    z = np.dot(W.transpose(),X)+b\n",
    "    yhat = sigmoid_fun(z)\n",
    "    return yhat\n",
    "\n",
    "def prediction(X_test,y_test,W,b):\n",
    "    yhat = calculate_yhat(X_test,W,b)\n",
    "    yhat = yhat.reshape(1, y_test.shape[1])\n",
    "    correct_prediction = round(((y_test==yhat).sum()/y_test.shape[1])*100,2)\n",
    "    print(f'Accuracy in test data {correct_prediction} %')\n",
    "\n",
    "\n",
    "def create_train_test_data(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath):\n",
    "    cat_X_train, cat_y_train = create_x_y_set(cat_train_filepath,height,width,cat)\n",
    "    dog_X_train, dog_y_train = create_x_y_set(dog_train_filepath,height,width,dog)\n",
    "    cat_X_test, cat_y_test = create_x_y_set(cat_test_filepath,height,width,cat)\n",
    "    dog_X_test, dog_y_test = create_x_y_set(dog_test_filepath,height,width,dog)\n",
    "    X_train = np.concatenate([cat_X_train,dog_X_train],axis=1)\n",
    "    y_train = np.concatenate([cat_y_train,dog_y_train],axis=1)\n",
    "    X_test = np.concatenate([cat_X_test,dog_X_train],axis=1)\n",
    "    y_test = np.concatenate([cat_y_test,dog_y_train],axis=1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath,W,b,lr):\n",
    "    X_train, X_test, y_train, y_test = create_train_test_data(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath)\n",
    "    for i in range(100):\n",
    "        yhat = calculate_yhat(X_train,W,b)\n",
    "        yhat = yhat.reshape(1, y_train.shape[1])\n",
    "        dz = partial_derivative_dz(yhat,y_train)\n",
    "        dw = partial_derivative_dw(dz,X_train)\n",
    "        db = partial_derivative_db(dz)\n",
    "        # adjust the weights\n",
    "        W = W - lr*dw\n",
    "        b = b - lr*db\n",
    "        bin_loss = binary_entropy_loss(yhat, y_train)\n",
    "        correct_prediction = round(((y_train==yhat).sum()/y_train.shape[1])*100,2)\n",
    "        print(f'epoch no - {i} binary_entropy_loss - {bin_loss} correct prediction - {correct_prediction} %')\n",
    "    return W,b\n",
    "\n",
    "def make_prediction(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath,W,b):\n",
    "    X_train, X_test, y_train, y_test = create_train_test_data(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath)\n",
    "    yhat = calculate_yhat(X_test,W,b)\n",
    "    yhat = yhat.reshape(1, y_test.shape[1])\n",
    "    correct_prediction = round(((y_test==yhat).sum()/y_test.shape[1])*100,2)\n",
    "    print(f'Test dataset accuracy- {correct_prediction} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no - 0 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 1 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 2 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 3 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 4 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 5 binary_entropy_loss - 8.073516543203278 correct prediction - 49.91 %\n",
      "epoch no - 6 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 7 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 8 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 9 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 10 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 11 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 12 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/_bs3cjyn0wx77s6vhtrjwn1m0000gn/T/ipykernel_53614/2905522531.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+ np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no - 13 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 14 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 15 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 16 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 17 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 18 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 19 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 20 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 21 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 22 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 23 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 24 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 25 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 26 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 27 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 28 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 29 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 30 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 31 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 32 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 33 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 34 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 35 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 36 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 37 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 38 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 39 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 40 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 41 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 42 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 43 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 44 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 45 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 46 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 47 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 48 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 49 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 50 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 51 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 52 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 53 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 54 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 55 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 56 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 57 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 58 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 59 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 60 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 61 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 62 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 63 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 64 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 65 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 66 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 67 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 68 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 69 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 70 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 71 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 72 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 73 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 74 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 75 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 76 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 77 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 78 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 79 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 80 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 81 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 82 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 83 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 84 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 85 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 86 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 87 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 88 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 89 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 90 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 91 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 92 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 93 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 94 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 95 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 96 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 97 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 98 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "epoch no - 99 binary_entropy_loss - 1.0000000494736474e-07 correct prediction - 100.0 %\n",
      "Test dataset accuracy- 79.89 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W,b = train_model(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath,W,b,lr)\n",
    "make_prediction(cat_train_filepath,dog_train_filepath,cat_test_filepath,dog_test_filepath,W,b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
